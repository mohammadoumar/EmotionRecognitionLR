{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oHFCsV0z-Jw"
   },
   "source": [
    "# Finetune Comics dataset for emotion classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr7rB3szzhtx"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "giM74oK1rRIH",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell only once to install LLaMA-Factory\n",
    "\n",
    "# %cd ..\n",
    "# %rm -rf LLaMA-Factory\n",
    "# !git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "# %cd LLaMA-Factory\n",
    "# %ls\n",
    "# !pip install -e .[torch,bitsandbytes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y pydantic\n",
    "# !pip install pydantic==1.10.9 # \n",
    "\n",
    "# !pip uninstall -y gradio\n",
    "# !pip install gradio==3.48.0\n",
    "\n",
    "# !pip uninstall -y bitsandbytes\n",
    "# !pip install --upgrade bitsandbytes\n",
    "\n",
    "# !pip install tqdm\n",
    "# !pip install ipywidgets\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# Restart kernel afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import subprocess\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc\n",
    "from sklearn.metrics import classification_report\n",
    "# from utils.post_processing import post_process_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:    \n",
    "    assert torch.cuda.is_available() is True\n",
    "    \n",
    "except AssertionError:\n",
    "    \n",
    "    print(\"Please set up a GPU before using LLaMA Factory...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AM_DIR = os.path.abspath(os.path.join(os.path.join(ROOT_DIR, os.pardir), \"am_work\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = os.path.join(ROOT_DIR, \"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_FACTORY_DIR = os.path.join(AM_DIR, \"coling_2025/LLaMA-Factory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq/am_work/coling_2025/LLaMA-Factory'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLAMA_FACTORY_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"unsloth/Qwen2-7B-Instruct-bnb-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"er\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAGS = 1\n",
    "# TAGS = \"wtags\" if TAGS == 1 else \"wotags\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTEXT = \"essay\" # essay or paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.path.join(ROOT_DIR, \"finetuned_models\", f\"\"\"comics_{TASK}_{BASE_MODEL.split(\"/\")[1]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_Qwen2-7B-Instruct-bnb-4bit'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeYs5Lz-QJYk"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# *** TRAIN DATASET NAME *** #\n",
    "\n",
    "train_dataset_name = f\"\"\"comics_utterance_train.json\"\"\"\n",
    "test_dataset_name = f\"\"\"comics_utterance_test.json\"\"\"\n",
    "\n",
    "#train_dataset_name = f\"\"\"PE_{TASK}_{CONTEXT}_train.json\"\"\"\n",
    "train_dataset_file = os.path.join(DATASET_DIR, train_dataset_name)\n",
    "\n",
    "# *** TEST DATASET NAME *** #\n",
    "\n",
    "#test_dataset_name = f\"\"\"PE_{TASK}_{CONTEXT}_test.json\"\"\"\n",
    "test_dataset_file = os.path.join(DATASET_DIR, test_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Utilisateurs/umushtaq/er_work/datasets/comics_utterance_train.json',\n",
       " '/Utilisateurs/umushtaq/er_work/datasets/comics_utterance_test.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_file, test_dataset_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgR3UFhB0Ifq"
   },
   "source": [
    "## Fine-tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(ROOT_DIR, \"ft_arg_files\")):\n",
    "    os.mkdir(os.path.join(ROOT_DIR, \"ft_arg_files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# *** TRAIN FILE ***\n",
    "\n",
    "# model_name = f\"\"\"{train_dataset_name.split(\".\")[0].split(\"train\")[0]}{BASE_MODEL.split(\"/\")[1]}\"\"\"\n",
    "\n",
    "train_file = os.path.join(ROOT_DIR, \"ft_arg_files\", f\"\"\"{train_dataset_name.split(\".\")[0].split(\"train\")[0]}{BASE_MODEL.split(\"/\")[1]}.json\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_info_line =  {\n",
    "  \"file_name\": f\"{train_dataset_file}\",\n",
    "  \"columns\": {\n",
    "    \"prompt\": \"instruction\",\n",
    "    \"query\": \"input\",\n",
    "    \"response\": \"output\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': '/Utilisateurs/umushtaq/er_work/datasets/comics_utterance_train.json',\n",
       " 'columns': {'prompt': 'instruction', 'query': 'input', 'response': 'output'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_info_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(LLAMA_FACTORY_DIR, \"data/dataset_info.json\"), \"r\") as jsonFile:\n",
    "    data = json.load(jsonFile)\n",
    "\n",
    "data[\"comics_er\"] = dataset_info_line\n",
    "\n",
    "with open(os.path.join(LLAMA_FACTORY_DIR, \"data/dataset_info.json\"), \"w\") as jsonFile:\n",
    "    json.dump(data, jsonFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "CS0Qk5OR0i4Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = dict(\n",
    "  stage=\"sft\",                           # do supervised fine-tuning\n",
    "  do_train=True,\n",
    "  model_name_or_path=BASE_MODEL,         # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  dataset=\"comics_er\",           # use alpaca and identity datasets\n",
    "  template=\"qwen\",                     # use llama3 prompt template\n",
    "  finetuning_type=\"lora\",                # use LoRA adapters to save memory\n",
    "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
    "  output_dir=OUTPUT_DIR,                 # the path to save LoRA adapters\n",
    "  overwrite_output_dir=True,             # overrides existing output contents\n",
    "  per_device_train_batch_size=2,         # the batch size\n",
    "  gradient_accumulation_steps=4,         # the gradient accumulation steps\n",
    "  lr_scheduler_type=\"cosine\",            # use cosine learning rate scheduler\n",
    "  logging_steps=10,                      # log every 10 steps\n",
    "  warmup_ratio=0.1,                      # use warmup scheduler\n",
    "  save_steps=3000,                       # save checkpoint every 1000 steps\n",
    "  learning_rate=5e-5,                    # the learning rate\n",
    "  num_train_epochs=NB_EPOCHS,            # the epochs of training\n",
    "  max_samples=5000,                       # use 500 examples in each dataset\n",
    "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
    "  quantization_bit=4,                    # use 4-bit QLoRA\n",
    "  loraplus_lr_ratio=16.0,                # use LoRA+ algorithm with lambda=16.0\n",
    "  fp16=True,                             # use float16 mixed precision training\n",
    "  report_to=\"none\"                       # discards wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(args, open(train_file, \"w\", encoding=\"utf-8\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq/er_work/ft_arg_files/comics_utterance_Qwen2-7B-Instruct-bnb-4bit.json'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.Popen([\"llamafactory-cli\", \"train\", train_file], cwd=LLAMA_FACTORY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 20:36:11 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:28716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0917 20:36:12.734000 139658647946560 torch/distributed/run.py:757] \n",
      "W0917 20:36:12.734000 139658647946560 torch/distributed/run.py:757] *****************************************\n",
      "W0917 20:36:12.734000 139658647946560 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0917 20:36:12.734000 139658647946560 torch/distributed/run.py:757] *****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 20:36:21 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "09/17/2024 20:36:21 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "09/17/2024 20:36:21 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n",
      "09/17/2024 20:36:21 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "09/17/2024 20:36:21 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "09/17/2024 20:36:21 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 20:36:22,157 >> loading file vocab.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 20:36:22,157 >> loading file merges.txt from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 20:36:22,157 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 20:36:22,158 >> loading file added_tokens.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 20:36:22,158 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 20:36:22,158 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 20:36:22 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "09/17/2024 20:36:22 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "09/17/2024 20:36:22 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/er_work/datasets/comics_utterance_train.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2513] 2024-09-17 20:36:22,408 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Converting format of dataset: 100%|██████████| 3506/3506 [00:00<00:00, 47574.50 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 20:36:23 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/er_work/datasets/comics_utterance_train.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████| 3506/3506 [00:03<00:00, 1030.49 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training example:\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 14374, 1446, 525, 458, 6203, 304, 5748, 5956, 18320, 13, 1446, 525, 2661, 458, 8621, 4160, 578, 504, 264, 19724, 2311, 43810, 553, 366, 1381, 1472, 1381, 29, 9492, 13, 4615, 3383, 374, 311, 48129, 1817, 21532, 681, 438, 825, 476, 803, 279, 2701, 19772, 6846, 25, 330, 10743, 261, 1, 320, 1093, 701, 330, 4839, 70, 590, 1, 320, 17625, 701, 330, 90371, 1, 320, 11419, 701, 330, 59665, 2090, 1, 320, 7778, 701, 330, 23043, 9671, 1, 320, 59782, 8, 476, 330, 79771, 1, 320, 26145, 568, 1446, 1969, 470, 264, 1140, 315, 19772, 6846, 304, 2701, 4718, 3561, 25, 5212, 1607, 22504, 5956, 16833, 788, 4383, 73353, 4790, 320, 495, 11583, 330, 73353, 4790, 320, 495, 9940, 2503, 330, 73353, 4790, 320, 495, 8, 92446, 1380, 1817, 2392, 330, 73353, 16833, 320, 495, 9940, 374, 12575, 553, 825, 16144, 803, 315, 279, 2701, 94775, 19772, 536, 9201, 25, 330, 1093, 497, 330, 17625, 497, 330, 11419, 497, 330, 7778, 497, 330, 59782, 1, 476, 330, 26145, 3263, 4710, 14374, 5692, 374, 279, 21532, 681, 504, 264, 19724, 2311, 25, 366, 1381, 36495, 915, 14985, 18590, 5146, 72477, 73645, 11447, 15645, 5627, 69902, 26055, 1381, 29, 151645, 198, 151644, 77091, 198, 4913, 1607, 22504, 5956, 16833, 788, 4383, 11419, 497, 330, 59782, 92446, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "### You are an expert in Emotion Analysis. You are given an utternace from a comic book enclosed by <UT></UT> tags. Your task is to classify each utterance as one or more the following emotion classes: \"Anger\" (AN), \"Disgust\" (DI), \"Fear\" (FE), \"Sadness\" (SA), \"Surprise\" (SU) or \"Joy\" (JO). You must return a list of emotion classes in following JSON format: {\"list_emotion_classes\": [\"emotion_class (str)\", \"emotion_class (str)\" ... \"emotion_class (str)\"]} where each element \"emotion_classes (str)\" is replaced by one ore more of the following abbreviated emotion class labels: \"AN\", \"DI\", \"FE\", \"SA\", \"SU\" or \"JO\". \n",
      "\n",
      "### Here is the utterance from a comic book: <UT>DID YOU HAVE TO ELECTROCUTE HER SO HARD?</UT><|im_end|>\n",
      "<|im_start|>assistant\n",
      "{\"list_emotion_classes\": [\"FE\", \"SU\"]}<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4913, 1607, 22504, 5956, 16833, 788, 4383, 11419, 497, 330, 59782, 92446, 151645]\n",
      "labels:\n",
      "{\"list_emotion_classes\": [\"FE\", \"SU\"]}<|im_end|>\n",
      "09/17/2024 20:36:27 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "09/17/2024 20:36:27 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "09/17/2024 20:36:27 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "09/17/2024 20:36:27 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-09-17 20:36:27,058 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-17 20:36:27,061 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"unsloth/Qwen2-7B-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[WARNING|quantization_config.py:398] 2024-09-17 20:36:27,170 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3678] 2024-09-17 20:36:27,179 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/model.safetensors\n",
      "[INFO|modeling_utils.py:1606] 2024-09-17 20:36:28,401 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1038] 2024-09-17 20:36:28,404 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"pad_token_id\": 151643\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4507] 2024-09-17 20:37:23,713 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4515] 2024-09-17 20:37:23,713 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at unsloth/Qwen2-7B-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:993] 2024-09-17 20:37:23,828 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-09-17 20:37:23,829 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"max_length\": 32768,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 20:37:23 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "09/17/2024 20:37:23 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "09/17/2024 20:37:23 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "09/17/2024 20:37:23 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "09/17/2024 20:37:23 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,o_proj,q_proj,down_proj,gate_proj,k_proj,up_proj\n",
      "09/17/2024 20:37:24 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n",
      "09/17/2024 20:37:24 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "09/17/2024 20:37:24 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "09/17/2024 20:37:24 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "09/17/2024 20:37:24 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "09/17/2024 20:37:24 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,o_proj,down_proj,q_proj,k_proj,gate_proj,v_proj\n",
      "09/17/2024 20:37:24 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "09/17/2024 20:37:24 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:648] 2024-09-17 20:37:24,784 >> Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 20:37:25 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2134] 2024-09-17 20:37:25,248 >> ***** Running training *****\n",
      "[INFO|trainer.py:2135] 2024-09-17 20:37:25,261 >>   Num examples = 3,506\n",
      "[INFO|trainer.py:2136] 2024-09-17 20:37:25,261 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2137] 2024-09-17 20:37:25,261 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2140] 2024-09-17 20:37:25,261 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2141] 2024-09-17 20:37:25,261 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2142] 2024-09-17 20:37:25,261 >>   Total optimization steps = 657\n",
      "[INFO|trainer.py:2143] 2024-09-17 20:37:25,264 >>   Number of trainable parameters = 20,185,088\n",
      "  2%|▏         | 10/657 [00:26<28:37,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5513, 'grad_norm': 1.1023401021957397, 'learning_rate': 7.5757575757575764e-06, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 20/657 [00:53<28:03,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2996, 'grad_norm': 1.0976431369781494, 'learning_rate': 1.5151515151515153e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 30/657 [01:19<27:48,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2764, 'grad_norm': 1.1571595668792725, 'learning_rate': 2.272727272727273e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 40/657 [01:46<27:13,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2651, 'grad_norm': 1.1237519979476929, 'learning_rate': 3.0303030303030306e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 50/657 [02:12<26:35,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2523, 'grad_norm': 0.5649328827857971, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 60/657 [02:39<26:38,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2438, 'grad_norm': 0.552849292755127, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 70/657 [03:05<25:39,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2467, 'grad_norm': 1.1220053434371948, 'learning_rate': 4.999434882941783e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 80/657 [03:31<25:16,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.245, 'grad_norm': 0.7973174452781677, 'learning_rate': 4.993080249767159e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 90/657 [03:58<25:01,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2366, 'grad_norm': 0.9576094150543213, 'learning_rate': 4.979682598982912e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 100/657 [04:24<24:36,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2421, 'grad_norm': 1.114205241203308, 'learning_rate': 4.959279779306672e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 110/657 [04:51<24:09,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2407, 'grad_norm': 0.6632283926010132, 'learning_rate': 4.931929429243368e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 120/657 [05:17<23:47,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2234, 'grad_norm': 0.5057310461997986, 'learning_rate': 4.8977088142549285e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 130/657 [05:44<23:02,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2275, 'grad_norm': 0.4836157262325287, 'learning_rate': 4.856714608483312e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 140/657 [06:10<22:35,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2254, 'grad_norm': 0.6469013690948486, 'learning_rate': 4.8090626216434944e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 150/657 [06:36<22:29,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2149, 'grad_norm': 1.969303011894226, 'learning_rate': 4.754887471857969e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 160/657 [07:03<22:14,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2551, 'grad_norm': 1.0166857242584229, 'learning_rate': 4.694342205356988e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 170/657 [07:29<21:25,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2194, 'grad_norm': 1.0679407119750977, 'learning_rate': 4.627597864118919e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 180/657 [07:56<20:55,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2377, 'grad_norm': 1.2205135822296143, 'learning_rate': 4.554843002672129e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 190/657 [08:22<20:30,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2251, 'grad_norm': 0.7987896203994751, 'learning_rate': 4.476283155423465e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 200/657 [08:48<20:14,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.235, 'grad_norm': 0.7591536045074463, 'learning_rate': 4.3921402560181175e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 210/657 [09:15<19:41,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.222, 'grad_norm': 0.6437086462974548, 'learning_rate': 4.302652010371205e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 220/657 [09:41<19:15,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2074, 'grad_norm': 0.789050817489624, 'learning_rate': 4.208071225142282e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 230/657 [10:08<18:55,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1949, 'grad_norm': 0.6989786028862, 'learning_rate': 4.108665093549844e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 240/657 [10:34<18:22,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2118, 'grad_norm': 0.6337319612503052, 'learning_rate': 4.0047144405434175e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 250/657 [11:01<17:53,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1844, 'grad_norm': 0.5998358726501465, 'learning_rate': 3.896512929465659e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 260/657 [11:27<17:29,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2047, 'grad_norm': 0.7406709790229797, 'learning_rate': 3.7843662324456466e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 270/657 [11:53<16:56,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2071, 'grad_norm': 1.0060415267944336, 'learning_rate': 3.668591166867035e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 280/657 [12:20<16:30,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1986, 'grad_norm': 1.3233298063278198, 'learning_rate': 3.5495148003505717e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 290/657 [12:46<16:04,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2183, 'grad_norm': 0.4204199016094208, 'learning_rate': 3.4274735267794245e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 300/657 [13:12<15:34,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1777, 'grad_norm': 0.4864746332168579, 'learning_rate': 3.3028121159775656e-05, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 310/657 [13:39<15:09,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1799, 'grad_norm': 0.9694530963897705, 'learning_rate': 3.1758827397259074e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 320/657 [14:05<14:47,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1873, 'grad_norm': 0.5777468681335449, 'learning_rate': 3.0470439768677116e-05, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 330/657 [14:31<14:20,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2075, 'grad_norm': 0.6403128504753113, 'learning_rate': 2.9166598003138766e-05, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 340/657 [14:58<13:56,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1824, 'grad_norm': 1.1541911363601685, 'learning_rate': 2.785098548809844e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 350/657 [15:24<13:22,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1875, 'grad_norm': 1.0687892436981201, 'learning_rate': 2.652731886368906e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 360/657 [15:51<13:01,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1956, 'grad_norm': 0.5424371361732483, 'learning_rate': 2.5199337523115418e-05, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 370/657 [16:17<12:43,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1954, 'grad_norm': 0.5226247906684875, 'learning_rate': 2.3870793048769537e-05, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 380/657 [16:44<12:15,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1603, 'grad_norm': 0.4974578619003296, 'learning_rate': 2.254543861391121e-05, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 390/657 [17:10<11:41,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2102, 'grad_norm': 0.7681999802589417, 'learning_rate': 2.1227018379854383e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 400/657 [17:36<11:15,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1968, 'grad_norm': 0.6189435720443726, 'learning_rate': 1.991925691861241e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 410/657 [18:03<10:52,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1895, 'grad_norm': 0.6123371720314026, 'learning_rate': 1.8625848690883686e-05, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 420/657 [18:29<10:22,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1894, 'grad_norm': 0.8038066625595093, 'learning_rate': 1.735044760910251e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 430/657 [18:56<09:58,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1818, 'grad_norm': 0.6552383303642273, 'learning_rate': 1.609665671503987e-05, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 440/657 [19:22<09:31,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1746, 'grad_norm': 0.5469232201576233, 'learning_rate': 1.4868018001115166e-05, 'epoch': 2.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 450/657 [19:48<09:06,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1246, 'grad_norm': 0.4772845208644867, 'learning_rate': 1.3668002404174047e-05, 'epoch': 2.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 460/657 [20:15<08:38,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1173, 'grad_norm': 0.6368279457092285, 'learning_rate': 1.2500000000000006e-05, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 470/657 [20:41<08:13,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0953, 'grad_norm': 0.7478751540184021, 'learning_rate': 1.136731042626073e-05, 'epoch': 2.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 480/657 [21:07<07:48,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0988, 'grad_norm': 0.8873948454856873, 'learning_rate': 1.027313356094443e-05, 'epoch': 2.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 490/657 [21:34<07:19,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1025, 'grad_norm': 0.7643134593963623, 'learning_rate': 9.220560482619956e-06, 'epoch': 2.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 500/657 [22:00<06:53,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0899, 'grad_norm': 0.7233508229255676, 'learning_rate': 8.21256473805811e-06, 'epoch': 2.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 510/657 [22:26<06:26,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1002, 'grad_norm': 0.9916441440582275, 'learning_rate': 7.251993941883428e-06, 'epoch': 2.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 520/657 [22:53<06:00,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1052, 'grad_norm': 1.2063912153244019, 'learning_rate': 6.3415617319875716e-06, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 530/657 [23:19<05:34,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1063, 'grad_norm': 0.6956425905227661, 'learning_rate': 5.483840103430599e-06, 'epoch': 2.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 540/657 [23:46<05:10,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1091, 'grad_norm': 0.8245587944984436, 'learning_rate': 4.681252142486841e-06, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 550/657 [24:12<04:40,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1132, 'grad_norm': 0.853794276714325, 'learning_rate': 3.936065181362211e-06, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 560/657 [24:38<04:14,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1084, 'grad_norm': 0.8456055521965027, 'learning_rate': 3.2503843929207413e-06, 'epoch': 2.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 570/657 [25:05<03:49,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1011, 'grad_norm': 0.648411214351654, 'learning_rate': 2.6261468435155978e-06, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 580/657 [25:31<03:22,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1002, 'grad_norm': 0.6906623244285583, 'learning_rate': 2.065116020725433e-06, 'epoch': 2.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 590/657 [25:58<02:56,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1123, 'grad_norm': 1.2719172239303589, 'learning_rate': 1.5688768514553587e-06, 'epoch': 2.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 600/657 [26:24<02:31,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1025, 'grad_norm': 0.9647970199584961, 'learning_rate': 1.138831224476533e-06, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 610/657 [26:51<02:04,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1134, 'grad_norm': 0.6448526978492737, 'learning_rate': 7.761940300533399e-07, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 620/657 [27:17<01:37,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0991, 'grad_norm': 0.8042908310890198, 'learning_rate': 4.819897278462521e-07, 'epoch': 2.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 630/657 [27:43<01:11,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1026, 'grad_norm': 0.809659481048584, 'learning_rate': 2.5704945278623436e-07, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 640/657 [28:10<00:44,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1019, 'grad_norm': 0.8860611915588379, 'learning_rate': 1.0200866709657864e-07, 'epoch': 2.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 650/657 [28:36<00:18,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.101, 'grad_norm': 0.8921215534210205, 'learning_rate': 1.730536509532421e-08, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 657/657 [28:54<00:00,  2.63s/it][INFO|trainer.py:3503] 2024-09-17 21:06:20,289 >> Saving model checkpoint to /Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_Qwen2-7B-Instruct-bnb-4bit/checkpoint-657\n",
      "[INFO|configuration_utils.py:733] 2024-09-17 21:06:20,681 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-17 21:06:20,682 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"unsloth/Qwen2-7B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-09-17 21:06:21,729 >> tokenizer config file saved in /Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_Qwen2-7B-Instruct-bnb-4bit/checkpoint-657/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-09-17 21:06:21,730 >> Special tokens file saved in /Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_Qwen2-7B-Instruct-bnb-4bit/checkpoint-657/special_tokens_map.json\n",
      "[INFO|trainer.py:2394] 2024-09-17 21:06:24,002 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 657/657 [28:58<00:00,  2.65s/it]\n",
      "[INFO|trainer.py:3503] 2024-09-17 21:06:24,005 >> Saving model checkpoint to /Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_Qwen2-7B-Instruct-bnb-4bit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1738.738, 'train_samples_per_second': 6.049, 'train_steps_per_second': 0.378, 'train_loss': 0.1841895292701605, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-09-17 21:06:24,304 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-17 21:06:24,304 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"unsloth/Qwen2-7B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-09-17 21:06:25,317 >> tokenizer config file saved in /Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_Qwen2-7B-Instruct-bnb-4bit/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-09-17 21:06:25,318 >> Special tokens file saved in /Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_Qwen2-7B-Instruct-bnb-4bit/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =      2.9966\n",
      "  total_flos               = 101340307GF\n",
      "  train_loss               =      0.1842\n",
      "  train_runtime            =  0:28:58.73\n",
      "  train_samples_per_second =       6.049\n",
      "  train_steps_per_second   =       0.378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modelcard.py:449] 2024-09-17 21:06:25,641 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVNaC-xS5N40"
   },
   "source": [
    "## Inference on the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_Qwen2-7B-Instruct-bnb-4bit'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainer_log.jsonl',\n",
       " 'checkpoint-657',\n",
       " 'README.md',\n",
       " 'adapter_model.safetensors',\n",
       " 'adapter_config.json',\n",
       " 'tokenizer_config.json',\n",
       " 'special_tokens_map.json',\n",
       " 'added_tokens.json',\n",
       " 'vocab.json',\n",
       " 'merges.txt',\n",
       " 'tokenizer.json',\n",
       " 'training_args.bin',\n",
       " 'train_results.json',\n",
       " 'all_results.json',\n",
       " 'trainer_state.json']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "oh8H9A_25SF9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = dict(\n",
    "  model_name_or_path=BASE_MODEL, # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  adapter_name_or_path=OUTPUT_DIR,            # load the saved LoRA adapters\n",
    "  template=\"qwen\",                     # same to the one in training\n",
    "  finetuning_type=\"lora\",                  # same to the one in training\n",
    "  quantization_bit=4,                    # load 4-bit quantized model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 21:06:28,952 >> loading file vocab.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 21:06:28,954 >> loading file merges.txt from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 21:06:28,956 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 21:06:28,958 >> loading file added_tokens.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 21:06:28,960 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 21:06:28,962 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-09-17 21:06:29,131 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 21:06:29 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-09-17 21:06:29,256 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-17 21:06:29,260 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"unsloth/Qwen2-7B-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 21:06:29 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "09/17/2024 21:06:29 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "09/17/2024 21:06:29 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|quantization_config.py:398] 2024-09-17 21:06:29,523 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3678] 2024-09-17 21:06:29,527 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/model.safetensors\n",
      "[INFO|modeling_utils.py:1606] 2024-09-17 21:06:29,560 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1038] 2024-09-17 21:06:29,564 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"pad_token_id\": 151643\n",
      "}\n",
      "\n",
      "[INFO|quantizer_bnb_4bit.py:106] 2024-09-17 21:06:30,712 >> target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "[INFO|modeling_utils.py:4507] 2024-09-17 21:06:32,872 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4515] 2024-09-17 21:06:32,874 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at unsloth/Qwen2-7B-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:993] 2024-09-17 21:06:33,000 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Qwen2-7B-Instruct-bnb-4bit/snapshots/26ac6deb2f1f4929ecc7ece49178be31aa1e0755/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-09-17 21:06:33,001 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"max_length\": 32768,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 21:06:33 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "09/17/2024 21:06:33 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_Qwen2-7B-Instruct-bnb-4bit\n",
      "09/17/2024 21:06:33 - INFO - llamafactory.model.loader - all params: 7,635,801,600\n"
     ]
    }
   ],
   "source": [
    "model = ChatModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(test_dataset_file, \"r+\") as fh:\n",
    "    test_dataset = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_prompts = []\n",
    "test_grounds = []\n",
    "\n",
    "for sample in test_dataset:\n",
    "    test_prompts.append(\"\\nUser:\" + sample[\"instruction\"] + sample[\"input\"])\n",
    "    test_grounds.append(sample[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5bdd4b72ad427f9b7b07323ecb9d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1776 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_predictions = []\n",
    "\n",
    "for prompt in tqdm(test_prompts):\n",
    "\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = \"\"\n",
    "    \n",
    "    for new_text in model.stream_chat(messages):\n",
    "        #print(new_text, end=\"\", flush=True)\n",
    "        response += new_text\n",
    "        #print()\n",
    "    test_predictions.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    torch_gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIR, f\"\"\"comics_{TASK}_results_{NB_EPOCHS}.pickle\"\"\"), 'wb') as fh:\n",
    "    results_d = {\"ground_truths\": test_grounds,\n",
    "                 \"predictions\": test_predictions    \n",
    "        \n",
    "    }\n",
    "    pickle.dump(results_d, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIR, f\"\"\"comics_{TASK}_results_{NB_EPOCHS}.pickle\"\"\"), \"rb\") as fh:\n",
    "        \n",
    "        results = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounds = results[\"ground_truths\"]\n",
    "preds = results[\"predictions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grounds), len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounds_l = []\n",
    "\n",
    "for i in range(len(grounds)):\n",
    "    grounds_l.append(json.loads(grounds[i])['list_emotion_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grounds_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grounds_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [x[\"content\"] for x in preds]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_l = []\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    preds_l.append(json.loads(preds[i])['list_emotion_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, (i,j) in enumerate(zip(grounds_l, preds_l)):\n",
    "    if len(i) != len(j):\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def opposite_acc(component_type):\n",
    "\n",
    "#     if component_type == \"Premise\":\n",
    "#         return \"Claim\"\n",
    "#     elif component_type == \"Claim\":\n",
    "#         return \"Premise\"\n",
    "#     elif component_type == \"MajorClaim\":\n",
    "#         return \"Claim\"\n",
    "\n",
    "# def harmonize_preds_acc(grounds, preds):\n",
    "\n",
    "#     l1, l2 = len(preds), len(grounds)\n",
    "#     if l1 < l2:\n",
    "#         diff = l2 - l1\n",
    "#         preds = preds + [opposite_acc(x) for x in grounds[l1:]]\n",
    "#     else:\n",
    "#         preds = preds[:l2]\n",
    "        \n",
    "#     return preds \n",
    "\n",
    "# def post_process_acc(results):\n",
    "\n",
    "#     grounds = results[\"ground_truths\"]\n",
    "#     preds = results[\"predictions\"]\n",
    "    \n",
    "#     grounds = [json.loads(x)[\"component_types\"] for x in grounds]  \n",
    "    \n",
    "#     preds = [x[\"content\"] for x in preds]    \n",
    "#     preds = [json.loads(x)[\"component_types\"] for x in preds]\n",
    "    \n",
    "#     for i,(x,y) in enumerate(zip(grounds, preds)):\n",
    "    \n",
    "#         if len(x) != len(y):\n",
    "            \n",
    "#             preds[i] = harmonize_preds_acc(x, y)\n",
    "            \n",
    "#     task_preds = [item for row in preds for item in row]\n",
    "#     task_grounds = [item for row in grounds for item in row]\n",
    "\n",
    "#     return task_grounds, task_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Anger\" (AN), \"Disgust\" (DI), \"Fear\" (FE), \"Sadness\" (SA), \"Surprise\" (SU) or \"Joy\" (JO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opposite_acc(component_type):\n",
    "\n",
    "    if component_type == \"AN\":\n",
    "        return \"SU\"\n",
    "    elif component_type == \"DI\":\n",
    "        return \"JO\"\n",
    "    elif component_type == \"FE\":\n",
    "        return \"SA\"\n",
    "    elif component_type == \"SA\":\n",
    "        return \"AN\"\n",
    "    elif component_type == \"SU\":\n",
    "        return \"DI\"\n",
    "    elif component_type == \"JO\":\n",
    "        return \"FE\"\n",
    "    elif component_type == \"Neutral\":\n",
    "        return \"SA\"\n",
    "\n",
    "def harmonize_preds_acc(grounds, preds):\n",
    "\n",
    "    l1, l2 = len(preds), len(grounds)\n",
    "    if l1 < l2:\n",
    "        diff = l2 - l1\n",
    "        preds = preds + [opposite_acc(x) for x in grounds[l1:]]\n",
    "    else:\n",
    "        preds = preds[:l2]\n",
    "        \n",
    "    return preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(x,y) in enumerate(zip(grounds_l, preds_l)):\n",
    "\n",
    "    if len(x) != len(y):\n",
    "        \n",
    "        preds_l[i] = harmonize_preds_acc(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_preds = [item for row in preds_l for item in row]\n",
    "task_grounds = [item for row in grounds_l for item in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_grounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_grounds = ['Neutral' if x == ['Neutral'] else x for x in task_grounds]\n",
    "task_preds = ['Neutral' if x == ['Neutral'] else x for x in task_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: \n",
    "len(task_preds) == len(task_grounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(task_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(task_grounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task_grounds, task_preds = post_process_acc(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: \n",
    "#len(task_preds) == len(task_grounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(task_grounds, task_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"\"\"{OUTPUT_DIR}/classification_report_llama3.1_5.pickle\"\"\", 'wb') as fh:\n",
    "    \n",
    "    pickle.dump(classification_report(task_grounds, task_preds, output_dict=True), fh)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "          AN      0.508     0.565     0.535       614\n",
    "          DI      0.087     0.212     0.124        85\n",
    "          FE      0.337     0.418     0.373       407\n",
    "          JO      0.457     0.382     0.416       429\n",
    "     Neutral      0.321     0.070     0.115       129\n",
    "          SA      0.323     0.268     0.293       347\n",
    "          SU      0.428     0.377     0.400       486\n",
    "\n",
    "    accuracy                          0.394      2497\n",
    "   macro avg      0.352     0.327     0.322      2497\n",
    "weighted avg      0.406     0.394     0.393      2497\n",
    "\n",
    "gemma 2 epochs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
