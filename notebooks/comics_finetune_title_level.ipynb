{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oHFCsV0z-Jw"
   },
   "source": [
    "# Finetune PE dataset for ACC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr7rB3szzhtx"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "giM74oK1rRIH",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell only once to install LLaMA-Factory\n",
    "\n",
    "# %cd ..\n",
    "# %rm -rf LLaMA-Factory\n",
    "# !git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "# %cd LLaMA-Factory\n",
    "# %ls\n",
    "# !pip install -e .[torch,bitsandbytes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y pydantic\n",
    "# !pip install pydantic==1.10.9 # \n",
    "\n",
    "# !pip uninstall -y gradio\n",
    "# !pip install gradio==3.48.0\n",
    "\n",
    "# !pip uninstall -y bitsandbytes\n",
    "# !pip install --upgrade bitsandbytes\n",
    "\n",
    "# !pip install tqdm\n",
    "# !pip install ipywidgets\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# Restart kernel afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import subprocess\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc\n",
    "from sklearn.metrics import classification_report\n",
    "# from utils.post_processing import post_process_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:    \n",
    "    assert torch.cuda.is_available() is True\n",
    "    \n",
    "except AssertionError:\n",
    "    \n",
    "    print(\"Please set up a GPU before using LLaMA Factory...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AM_DIR = os.path.abspath(os.path.join(os.path.join(ROOT_DIR, os.pardir), \"am_work\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = os.path.join(ROOT_DIR, \"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_FACTORY_DIR = os.path.join(AM_DIR, \"coling_2025/LLaMA-Factory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq/am_work/coling_2025/LLaMA-Factory'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLAMA_FACTORY_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"unsloth/llama-3-8b-Instruct-bnb-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"er\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAGS = 1\n",
    "# TAGS = \"wtags\" if TAGS == 1 else \"wotags\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTEXT = \"essay\" # essay or paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.path.join(ROOT_DIR, \"finetuned_models\", f\"\"\"comics_{TASK}_{BASE_MODEL.split(\"/\")[1]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_llama-3-8b-Instruct-bnb-4bit'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeYs5Lz-QJYk"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# *** TRAIN DATASET NAME *** #\n",
    "\n",
    "train_dataset_name = f\"\"\"comics_utterance_train.json\"\"\"\n",
    "test_dataset_name = f\"\"\"comics_utterance_test.json\"\"\"\n",
    "\n",
    "#train_dataset_name = f\"\"\"PE_{TASK}_{CONTEXT}_train.json\"\"\"\n",
    "train_dataset_file = os.path.join(DATASET_DIR, train_dataset_name)\n",
    "\n",
    "# *** TEST DATASET NAME *** #\n",
    "\n",
    "#test_dataset_name = f\"\"\"PE_{TASK}_{CONTEXT}_test.json\"\"\"\n",
    "test_dataset_file = os.path.join(DATASET_DIR, test_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Utilisateurs/umushtaq/er_work/datasets/comics_train.json',\n",
       " '/Utilisateurs/umushtaq/er_work/datasets/comics_test.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_file, test_dataset_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgR3UFhB0Ifq"
   },
   "source": [
    "## Fine-tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(ROOT_DIR, \"ft_arg_files\")):\n",
    "    os.mkdir(os.path.join(ROOT_DIR, \"ft_arg_files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# *** TRAIN FILE ***\n",
    "\n",
    "# model_name = f\"\"\"{train_dataset_name.split(\".\")[0].split(\"train\")[0]}{BASE_MODEL.split(\"/\")[1]}\"\"\"\n",
    "\n",
    "train_file = os.path.join(ROOT_DIR, \"ft_arg_files\", f\"\"\"{train_dataset_name.split(\".\")[0].split(\"train\")[0]}{BASE_MODEL.split(\"/\")[1]}.json\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_info_line =  {\n",
    "  \"file_name\": f\"{train_dataset_file}\",\n",
    "  \"columns\": {\n",
    "    \"prompt\": \"instruction\",\n",
    "    \"query\": \"input\",\n",
    "    \"response\": \"output\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': '/Utilisateurs/umushtaq/er_work/datasets/comics_train.json',\n",
       " 'columns': {'prompt': 'instruction', 'query': 'input', 'response': 'output'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_info_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(LLAMA_FACTORY_DIR, \"data/dataset_info.json\"), \"r\") as jsonFile:\n",
    "    data = json.load(jsonFile)\n",
    "\n",
    "data[\"comics_er\"] = dataset_info_line\n",
    "\n",
    "with open(os.path.join(LLAMA_FACTORY_DIR, \"data/dataset_info.json\"), \"w\") as jsonFile:\n",
    "    json.dump(data, jsonFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "CS0Qk5OR0i4Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = dict(\n",
    "  stage=\"sft\",                           # do supervised fine-tuning\n",
    "  do_train=True,\n",
    "  model_name_or_path=BASE_MODEL,         # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  dataset=\"comics_er\",           # use alpaca and identity datasets\n",
    "  template=\"llama3\",                     # use llama3 prompt template\n",
    "  finetuning_type=\"lora\",                # use LoRA adapters to save memory\n",
    "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
    "  output_dir=OUTPUT_DIR,                 # the path to save LoRA adapters\n",
    "  overwrite_output_dir=True,             # overrides existing output contents\n",
    "  per_device_train_batch_size=2,         # the batch size\n",
    "  gradient_accumulation_steps=4,         # the gradient accumulation steps\n",
    "  lr_scheduler_type=\"cosine\",            # use cosine learning rate scheduler\n",
    "  logging_steps=10,                      # log every 10 steps\n",
    "  warmup_ratio=0.1,                      # use warmup scheduler\n",
    "  save_steps=3000,                       # save checkpoint every 1000 steps\n",
    "  learning_rate=5e-5,                    # the learning rate\n",
    "  num_train_epochs=NB_EPOCHS,            # the epochs of training\n",
    "  max_samples=2000,                       # use 500 examples in each dataset\n",
    "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
    "  quantization_bit=4,                    # use 4-bit QLoRA\n",
    "  loraplus_lr_ratio=16.0,                # use LoRA+ algorithm with lambda=16.0\n",
    "  fp16=True,                             # use float16 mixed precision training\n",
    "  report_to=\"none\"                       # discards wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(args, open(train_file, \"w\", encoding=\"utf-8\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq/er_work/ft_arg_files/comics_llama-3-8b-Instruct-bnb-4bit.json'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.Popen([\"llamafactory-cli\", \"train\", train_file], cwd=LLAMA_FACTORY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 13:54:16 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:29516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0917 13:54:17.702000 140510096708928 torch/distributed/run.py:757] \n",
      "W0917 13:54:17.702000 140510096708928 torch/distributed/run.py:757] *****************************************\n",
      "W0917 13:54:17.702000 140510096708928 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0917 13:54:17.702000 140510096708928 torch/distributed/run.py:757] *****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 13:54:26 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "09/17/2024 13:54:26 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "09/17/2024 13:54:26 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n",
      "09/17/2024 13:54:26 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "09/17/2024 13:54:26 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "09/17/2024 13:54:26 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 13:54:26,960 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 13:54:26,960 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 13:54:26,961 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 13:54:26,961 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-09-17 13:54:27,427 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 13:54:27 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "09/17/2024 13:54:27 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/er_work/datasets/comics_train.json...\n",
      "09/17/2024 13:54:27 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "09/17/2024 13:54:28 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/er_work/datasets/comics_train.json...\n",
      "training example:\n",
      "input_ids:\n",
      "[128000, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 5867, 6082, 18825, 13, 1472, 527, 2728, 279, 36815, 315, 264, 20303, 2363, 902, 5727, 49926, 3752, 22256, 3095, 44910, 555, 366, 1406, 1500, 1406, 29, 9681, 13, 4718, 3465, 374, 311, 49229, 1855, 22256, 685, 304, 279, 20303, 36815, 439, 389, 279, 2768, 20356, 6989, 25, 330, 10976, 261, 1, 320, 1111, 705, 330, 4944, 70, 592, 1, 320, 18091, 705, 330, 91471, 1, 320, 11673, 705, 330, 60765, 2136, 1, 320, 7934, 705, 330, 23912, 9868, 1, 320, 60882, 8, 477, 330, 80871, 1, 320, 27237, 570, 1472, 2011, 471, 264, 1160, 315, 20356, 6989, 11, 26549, 315, 3160, 220, 1958, 11, 304, 2768, 4823, 3645, 25, 5324, 1638, 23319, 6082, 17255, 794, 79786, 74453, 17255, 320, 496, 8, 8073, 4482, 74453, 17255, 320, 496, 8, 1365, 2564, 4482, 74453, 17255, 320, 496, 8, 1365, 14316, 1405, 1855, 2449, 330, 74453, 17255, 320, 496, 10143, 374, 12860, 555, 832, 16536, 810, 315, 279, 2768, 95875, 20356, 538, 9382, 25, 330, 1111, 498, 330, 18091, 498, 330, 11673, 498, 330, 7934, 498, 330, 60882, 1, 477, 330, 27237, 3343, 4815, 14711, 5810, 374, 279, 20303, 36815, 25, 366, 1406, 16, 66430, 28059, 409, 61423, 856, 45664, 1051, 4018, 4005, 1406, 16, 1822, 1406, 17, 66430, 12550, 326, 574, 43578, 4831, 27043, 88, 7006, 1418, 11039, 264, 3854, 315, 27840, 67487, 4005, 1406, 17, 1822, 1406, 18, 29, 3112, 1457, 11, 304, 6570, 5330, 39, 15836, 1981, 524, 1406, 18, 1822, 1406, 19, 1822, 358, 6, 4592, 81009, 15334, 0, 12890, 445, 35635, 0, 871, 81561, 1406, 19, 1822, 1406, 20, 1822, 10245, 58363, 45, 17773, 1198, 19708, 32740, 56, 7780, 25784, 4031, 445, 34708, 16202, 6654, 2089, 1242, 13880, 5257, 70747, 0, 27705, 1406, 20, 1822, 1406, 21, 1822, 11155, 4276, 6654, 2089, 1242, 13880, 1981, 27705, 1406, 21, 1822, 1406, 22, 1822, 1981, 5257, 73957, 13, 27705, 1406, 22, 1822, 1406, 23, 29, 2181, 596, 89994, 4005, 1406, 23, 1822, 1406, 24, 1822, 358, 59127, 3247, 32740, 5604, 16202, 49364, 16691, 4716, 18725, 330, 12343, 56, 4695, 45136, 330, 4015, 1899, 10155, 13, 11155, 82794, 15334, 30, 27705, 1406, 24, 1822, 1406, 605, 29, 30854, 30543, 88, 30, 4427, 14363, 1847, 27147, 1406, 605, 1822, 1406, 806, 67153, 568, 596, 19804, 11, 568, 2011, 617, 1981, 524, 1406, 806, 1822, 1406, 717, 37595, 309, 1800, 13, 358, 1288, 617, 25720, 433, 704, 3196, 389, 12343, 56, 4695, 45136, 13575, 7056, 1981, 524, 1406, 717, 1822, 1406, 1032, 1822, 1229, 81100, 13, 19102, 15334, 68609, 1507, 4230, 5338, 54026, 445, 2390, 9109, 30, 27705, 1406, 1032, 1822, 1406, 975, 1822, 358, 1981, 816, 19657, 39, 13, 27705, 1406, 975, 1822, 1406, 868, 1822, 362, 480, 32401, 362, 23769, 21592, 91688, 13, 7432, 1112, 32364, 17773, 82982, 66226, 26925, 1863, 13, 27705, 1406, 868, 1822, 1406, 845, 1822, 34007, 13953, 89994, 66873, 330, 2891, 715, 1210, 27705, 1406, 845, 1822, 1406, 1114, 29, 1548, 3287, 956, 1524, 20753, 311, 1005, 264, 69649, 643, 1863, 12700, 836, 1981, 524, 1406, 1114, 1822, 1406, 972, 29, 1981, 1606, 568, 6944, 757, 311, 1440, 568, 596, 832, 3094, 8469, 315, 757, 4005, 1406, 972, 1822, 1406, 777, 29, 13246, 304, 423, 14451, 691, 4005, 1406, 777, 1822, 1406, 508, 29, 2244, 37922, 19828, 1971, 1669, 4005, 1406, 508, 1822, 1406, 1691, 29, 2244, 72413, 46, 3791, 46, 4005, 1406, 1691, 1822, 1406, 1313, 29, 9241, 568, 9670, 279, 2324, 315, 1057, 7491, 69327, 1494, 8385, 3131, 568, 15393, 568, 574, 4967, 264, 11659, 56, 2198, 3143, 4932, 4005, 1406, 1313, 1822, 1406, 1419, 66928, 3194, 856, 4333, 4005, 1406, 1419, 1822, 1406, 1187, 66928, 3194, 889, 358, 3463, 568, 574, 4005, 1406, 1187, 1822, 1406, 914, 29, 1548, 6520, 38040, 757, 1457, 11, 1093, 264, 20457, 11, 439, 358, 3221, 279, 6381, 1584, 4005, 1406, 914, 1822, 1406, 1627, 29, 5159, 9131, 706, 2744, 1027, 922, 14324, 1274, 11, 719, 17958, 1981, 524, 1406, 1627, 1822, 1406, 1544, 29, 1981, 358, 2846, 16984, 832, 1938, 568, 2643, 5622, 757, 13, 1283, 706, 682, 856, 7512, 11, 719, 813, 4059, 706, 912, 77549, 11, 20444, 10705, 1981, 524, 1406, 1544, 1822, 1406, 1591, 66928, 617, 435, 21176, 4005, 1406, 1591, 1822, 1406, 1682, 66928, 2846, 16984, 358, 2834, 956, 387, 3025, 311, 3665, 7182, 323, 420, 9131, 690, 842, 323, 358, 3358, 5249, 856, 6699, 304, 279, 5015, 4005, 1406, 1682, 1822, 1406, 966, 29, 4516, 358, 1205, 311, 4034, 832, 810, 72297, 1981, 524, 1406, 966, 1822, 1406, 2148, 29, 1981, 311, 1304, 856, 386, 5358, 5644, 4005, 1406, 2148, 1822, 1406, 843, 94476, 1304, 757, 356, 8021, 3651, 350, 2716, 15544, 1753, 4005, 1406, 843, 1822, 1406, 1644, 94476, 1304, 757, 264, 17661, 315, 279, 76131, 4005, 1406, 1644, 1822, 1406, 1958, 29, 81663, 11, 7837, 36, 2006, 11, 29433, 13, 25404, 4031, 1981, 524, 1406, 1958, 29, 128009, 128006, 78191, 128007, 5018, 1638, 23319, 6082, 17255, 794, 79786, 1111, 498, 330, 11673, 498, 330, 60882, 8073, 4482, 1111, 498, 330, 11673, 498, 330, 7934, 8073, 4482, 1111, 498, 330, 11673, 498, 330, 7934, 8073, 4482, 11673, 498, 330, 7934, 498, 330, 60882, 8073, 4482, 1111, 498, 330, 27237, 8073, 4482, 27237, 8073, 4482, 27237, 8073, 4482, 60882, 8073, 4482, 1111, 498, 330, 60882, 8073, 4482, 1111, 498, 330, 60882, 8073, 4482, 11673, 498, 330, 60882, 8073, 4482, 1111, 498, 330, 18091, 498, 330, 7934, 498, 330, 60882, 8073, 4482, 11673, 498, 330, 7934, 8073, 4482, 11673, 498, 330, 60882, 498, 330, 27237, 8073, 4482, 88007, 8073, 4482, 88007, 8073, 4482, 1111, 498, 330, 18091, 498, 330, 60882, 8073, 4482, 1111, 498, 330, 18091, 8073, 4482, 1111, 498, 330, 18091, 8073, 4482, 1111, 498, 330, 18091, 8073, 4482, 1111, 498, 330, 18091, 8073, 4482, 1111, 498, 330, 18091, 498, 330, 7934, 8073, 4482, 7934, 8073, 4482, 7934, 8073, 4482, 11673, 498, 330, 7934, 8073, 4482, 1111, 498, 330, 18091, 498, 330, 7934, 8073, 4482, 11673, 498, 330, 7934, 8073, 4482, 11673, 8073, 4482, 11673, 498, 330, 7934, 8073, 4482, 88007, 8073, 4482, 88007, 8073, 4482, 88007, 8073, 4482, 27237, 8073, 4482, 27237, 1365, 14316, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "### You are an expert in Emotion Analysis. You are given the transcript of a comic book which contains numbered character utterances enclosed by <UT></UT> tags. Your task is to classify each utterance in the comic transcript as on the following emotion classes: \"Anger\" (AN), \"Disgust\" (DI), \"Fear\" (FE), \"Sadness\" (SA), \"Surprise\" (SU) or \"Joy\" (JO). You must return a list of emotion classes, strictly of length 34, in following JSON format: {\"list_emotion_classes\": [[\"emotion_classes (str)\"], [\"emotion_classes (str)\"]... [\"emotion_classes (str)\"]]} where each element \"emotion_classes (str)\" is replaced by one ore more of the following abbreviated emotion class labels: \"AN\", \"DI\", \"FE\", \"SA\", \"SU\" or \"JO\". \n",
      "\n",
      "### Here is the comic transcript: <UT1>In Rio de Janeiro my brakes were cut.</UT1><UT2>In Mexico l was slipped psilocybin while fighting a pack of jaguars.</UT2><UT3>And now, in SHANGHAI…</UT3><UT4>< I'VE GOT YOU! GO LIMP! > *</UT4><UT5>< THIS DOESN'T --MY POLYETHYLENE LINES ARE IMPOSSIBLE TO BREAK! ></UT5><UT6>< BUT NOT IMPOSSIBLE… ></UT6><UT7><… TO CUT. ></UT7><UT8>It's HIM.</UT8><UT9>< I KNOW THE POLICE ARE AFTER ME FOR MY \" SKYSPIDER \" STUNTS. BUT WHY YOU? ></UT9><UT10>Jealousy? Some sick game?</UT10><UT11>If he's HERE, he must have…</UT11><UT12>Dammit. I should have figured it out based on SKYSPIDER'S mask…</UT12><UT13>< QIU. HAVE YOU TRAINED ANYONE ELSE LATELY? ></UT13><UT14>< I… YEAH. ></UT14><UT15>< A GUY AROUND YOUR AGE. COULDN'T PLACE HIS ACCENT. ></UT15><UT16>< CALLED HIMSELF \"ANTON.\" ></UT16><UT17>He didn't even bother to use a DIFFERENT fake name…</UT17><UT18>… because he wants me to know he's one step ahead of me.</UT18><UT19>Like in DUBLIN.</UT19><UT20>Or METROPOLIS.</UT20><UT21>Or MOROCCO.</UT21><UT22>Where he ended the life of our master Ouahbi once he realized he was training a PSYCHOPATH.</UT22><UT23>I miss my friend.</UT23><UT24>I miss who I thought he was.</UT24><UT25>He haunts me now, like a ghost, as I near the finish line.</UT25><UT26>My mission has always been about saving people, but Anton…</UT26><UT27>… I'm afraid one day he might kill me. He has all my skills, but his mind has no distractions, whereas mine…</UT27><UT28>I have FEAR.</UT28><UT29>I'm afraid I won't be able to save myself and this mission will end and I'll join my parents in the ground.</UT29><UT30>So I need to visit one more MASTER…</UT30><UT31>… to make my MIND ready.</UT31><UT32>To make me COLD AND TERRIFYING.</UT32><UT33>To make me a creature of the NIGHT.</UT33><UT34>PLEASE, COME IN, MR. WAYNE…</UT34><|eot_id|><|start_header_id|>assistant<|end_header_id|>{\"list_emotion_classes\": [[\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"FE\", \"SA\", \"SU\"], [\"AN\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"AN\", \"SU\"], [\"AN\", \"SU\"], [\"FE\", \"SU\"], [\"AN\", \"DI\", \"SA\", \"SU\"], [\"FE\", \"SA\"], [\"FE\", \"SU\", \"JO\"], [\"Neutral\"], [\"Neutral\"], [\"AN\", \"DI\", \"SU\"], [\"AN\", \"DI\"], [\"AN\", \"DI\"], [\"AN\", \"DI\"], [\"AN\", \"DI\"], [\"AN\", \"DI\", \"SA\"], [\"SA\"], [\"SA\"], [\"FE\", \"SA\"], [\"AN\", \"DI\", \"SA\"], [\"FE\", \"SA\"], [\"FE\"], [\"FE\", \"SA\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"JO\"], [\"JO\"]]}<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 1638, 23319, 6082, 17255, 794, 79786, 1111, 498, 330, 11673, 498, 330, 60882, 8073, 4482, 1111, 498, 330, 11673, 498, 330, 7934, 8073, 4482, 1111, 498, 330, 11673, 498, 330, 7934, 8073, 4482, 11673, 498, 330, 7934, 498, 330, 60882, 8073, 4482, 1111, 498, 330, 27237, 8073, 4482, 27237, 8073, 4482, 27237, 8073, 4482, 60882, 8073, 4482, 1111, 498, 330, 60882, 8073, 4482, 1111, 498, 330, 60882, 8073, 4482, 11673, 498, 330, 60882, 8073, 4482, 1111, 498, 330, 18091, 498, 330, 7934, 498, 330, 60882, 8073, 4482, 11673, 498, 330, 7934, 8073, 4482, 11673, 498, 330, 60882, 498, 330, 27237, 8073, 4482, 88007, 8073, 4482, 88007, 8073, 4482, 1111, 498, 330, 18091, 498, 330, 60882, 8073, 4482, 1111, 498, 330, 18091, 8073, 4482, 1111, 498, 330, 18091, 8073, 4482, 1111, 498, 330, 18091, 8073, 4482, 1111, 498, 330, 18091, 8073, 4482, 1111, 498, 330, 18091, 498, 330, 7934, 8073, 4482, 7934, 8073, 4482, 7934, 8073, 4482, 11673, 498, 330, 7934, 8073, 4482, 1111, 498, 330, 18091, 498, 330, 7934, 8073, 4482, 11673, 498, 330, 7934, 8073, 4482, 11673, 8073, 4482, 11673, 498, 330, 7934, 8073, 4482, 88007, 8073, 4482, 88007, 8073, 4482, 88007, 8073, 4482, 27237, 8073, 4482, 27237, 1365, 14316, 128009]\n",
      "labels:\n",
      "{\"list_emotion_classes\": [[\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"FE\", \"SA\", \"SU\"], [\"AN\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"AN\", \"SU\"], [\"AN\", \"SU\"], [\"FE\", \"SU\"], [\"AN\", \"DI\", \"SA\", \"SU\"], [\"FE\", \"SA\"], [\"FE\", \"SU\", \"JO\"], [\"Neutral\"], [\"Neutral\"], [\"AN\", \"DI\", \"SU\"], [\"AN\", \"DI\"], [\"AN\", \"DI\"], [\"AN\", \"DI\"], [\"AN\", \"DI\"], [\"AN\", \"DI\", \"SA\"], [\"SA\"], [\"SA\"], [\"FE\", \"SA\"], [\"AN\", \"DI\", \"SA\"], [\"FE\", \"SA\"], [\"FE\"], [\"FE\", \"SA\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"JO\"], [\"JO\"]]}<|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-09-17 13:54:28,879 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-17 13:54:28,880 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[WARNING|quantization_config.py:398] 2024-09-17 13:54:28,960 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3678] 2024-09-17 13:54:28,963 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/model.safetensors\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:1606] 2024-09-17 13:54:29,020 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1038] 2024-09-17 13:54:29,041 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"pad_token_id\": 128255\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 13:54:28 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "09/17/2024 13:54:28 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "09/17/2024 13:54:28 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "09/17/2024 13:54:28 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4507] 2024-09-17 13:54:34,275 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4515] 2024-09-17 13:54:34,276 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:993] 2024-09-17 13:54:34,407 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-09-17 13:54:34,407 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 8192,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 13:54:34 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "09/17/2024 13:54:34 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "09/17/2024 13:54:34 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "09/17/2024 13:54:34 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "09/17/2024 13:54:34 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,v_proj,gate_proj,o_proj,down_proj,up_proj,k_proj\n",
      "09/17/2024 13:54:34 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "09/17/2024 13:54:34 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "09/17/2024 13:54:34 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "09/17/2024 13:54:34 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "09/17/2024 13:54:34 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,o_proj,gate_proj,k_proj,up_proj,q_proj,down_proj\n",
      "09/17/2024 13:54:35 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
      "09/17/2024 13:54:35 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
      "09/17/2024 13:54:35 - WARNING - llamafactory.train.callbacks - Previous trainer log in this folder will be deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:648] 2024-09-17 13:54:35,271 >> Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 13:54:35 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "09/17/2024 13:54:35 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2134] 2024-09-17 13:54:35,802 >> ***** Running training *****\n",
      "[INFO|trainer.py:2135] 2024-09-17 13:54:35,802 >>   Num examples = 22\n",
      "[INFO|trainer.py:2136] 2024-09-17 13:54:35,802 >>   Num Epochs = 50\n",
      "[INFO|trainer.py:2137] 2024-09-17 13:54:35,802 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2140] 2024-09-17 13:54:35,802 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2141] 2024-09-17 13:54:35,802 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2142] 2024-09-17 13:54:35,802 >>   Total optimization steps = 50\n",
      "[INFO|trainer.py:2143] 2024-09-17 13:54:35,806 >>   Number of trainable parameters = 20,971,520\n",
      " 20%|██        | 10/50 [01:09<04:36,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.693, 'grad_norm': 0.5575774908065796, 'learning_rate': 4.849231551964771e-05, 'epoch': 6.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [02:18<03:28,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3992, 'grad_norm': 1.3071566820144653, 'learning_rate': 3.7500000000000003e-05, 'epoch': 13.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [03:28<02:19,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1079, 'grad_norm': 0.6554983258247375, 'learning_rate': 2.0658795558326743e-05, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [04:38<01:09,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.01, 'grad_norm': 0.40512004494667053, 'learning_rate': 7.016504991533726e-06, 'epoch': 26.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [05:47<00:00,  6.95s/it][INFO|trainer.py:3503] 2024-09-17 14:00:23,643 >> Saving model checkpoint to /Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_llama-3-8b-Instruct-bnb-4bit/checkpoint-50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0014, 'grad_norm': 0.023578638210892677, 'learning_rate': 6.089874350439506e-08, 'epoch': 33.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-09-17 14:00:24,008 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-17 14:00:24,009 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-09-17 14:00:25,055 >> tokenizer config file saved in /Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_llama-3-8b-Instruct-bnb-4bit/checkpoint-50/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-09-17 14:00:25,056 >> Special tokens file saved in /Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_llama-3-8b-Instruct-bnb-4bit/checkpoint-50/special_tokens_map.json\n",
      "[INFO|trainer.py:2394] 2024-09-17 14:00:27,327 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 50/50 [05:51<00:00,  7.03s/it]\n",
      "[INFO|trainer.py:3503] 2024-09-17 14:00:27,329 >> Saving model checkpoint to /Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_llama-3-8b-Instruct-bnb-4bit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 351.5204, 'train_samples_per_second': 3.129, 'train_steps_per_second': 0.142, 'train_loss': 0.24229629039764405, 'epoch': 33.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-09-17 14:00:27,603 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-17 14:00:27,604 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-09-17 14:00:28,654 >> tokenizer config file saved in /Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_llama-3-8b-Instruct-bnb-4bit/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-09-17 14:00:28,655 >> Special tokens file saved in /Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_llama-3-8b-Instruct-bnb-4bit/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =    33.3333\n",
      "  total_flos               = 34450820GF\n",
      "  train_loss               =     0.2423\n",
      "  train_runtime            = 0:05:51.52\n",
      "  train_samples_per_second =      3.129\n",
      "  train_steps_per_second   =      0.142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modelcard.py:449] 2024-09-17 14:00:28,950 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVNaC-xS5N40"
   },
   "source": [
    "## Inference on the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_llama-3-8b-Instruct-bnb-4bit'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['checkpoint-1',\n",
       " 'README.md',\n",
       " 'adapter_model.safetensors',\n",
       " 'adapter_config.json',\n",
       " 'tokenizer_config.json',\n",
       " 'special_tokens_map.json',\n",
       " 'tokenizer.json',\n",
       " 'training_args.bin',\n",
       " 'train_results.json',\n",
       " 'all_results.json',\n",
       " 'trainer_state.json',\n",
       " 'comics_er_results_0.2.pickle',\n",
       " 'checkpoint-5',\n",
       " 'comics_er_results_5.pickle',\n",
       " 'checkpoint-20',\n",
       " 'comics_er_results_20.pickle',\n",
       " 'trainer_log.jsonl',\n",
       " 'checkpoint-50']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "oh8H9A_25SF9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = dict(\n",
    "  model_name_or_path=BASE_MODEL, # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  adapter_name_or_path=OUTPUT_DIR,            # load the saved LoRA adapters\n",
    "  template=\"llama3\",                     # same to the one in training\n",
    "  finetuning_type=\"lora\",                  # same to the one in training\n",
    "  quantization_bit=4,                    # load 4-bit quantized model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 14:00:33,952 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 14:00:33,954 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 14:00:33,956 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-09-17 14:00:33,958 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-09-17 14:00:34,198 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 14:00:34 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-09-17 14:00:34,316 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-09-17 14:00:34,318 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 14:00:34 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "09/17/2024 14:00:34 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "09/17/2024 14:00:34 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|quantization_config.py:398] 2024-09-17 14:00:34,417 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3678] 2024-09-17 14:00:34,420 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/model.safetensors\n",
      "[INFO|modeling_utils.py:1606] 2024-09-17 14:00:34,602 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1038] 2024-09-17 14:00:34,607 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"pad_token_id\": 128255\n",
      "}\n",
      "\n",
      "[INFO|quantizer_bnb_4bit.py:106] 2024-09-17 14:00:34,869 >> target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "[INFO|modeling_utils.py:4507] 2024-09-17 14:00:37,103 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4515] 2024-09-17 14:00:37,105 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:993] 2024-09-17 14:00:37,223 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/f296897830363557c84cc4a942c2cd1f91818ae4/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-09-17 14:00:37,224 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 8192,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/17/2024 14:00:37 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "09/17/2024 14:00:37 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/er_work/finetuned_models/comics_er_llama-3-8b-Instruct-bnb-4bit\n",
      "09/17/2024 14:00:37 - INFO - llamafactory.model.loader - all params: 8,051,232,768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:328] 2024-09-17 14:02:02,903 >> This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (8192). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "model = ChatModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(test_dataset_file, \"r+\") as fh:\n",
    "    test_dataset = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_prompts = []\n",
    "test_grounds = []\n",
    "\n",
    "for sample in test_dataset:\n",
    "    test_prompts.append(\"\\nUser:\" + sample[\"instruction\"] + sample[\"input\"])\n",
    "    test_grounds.append(sample[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50c814d7e5c4fab959e6ede6de548b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_predictions = []\n",
    "\n",
    "for prompt in tqdm(test_prompts):\n",
    "\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = \"\"\n",
    "    \n",
    "    for new_text in model.stream_chat(messages):\n",
    "        #print(new_text, end=\"\", flush=True)\n",
    "        response += new_text\n",
    "        #print()\n",
    "    test_predictions.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    torch_gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIR, f\"\"\"comics_{TASK}_results_{NB_EPOCHS}.pickle\"\"\"), 'wb') as fh:\n",
    "    results_d = {\"ground_truths\": test_grounds,\n",
    "                 \"predictions\": test_predictions    \n",
    "        \n",
    "    }\n",
    "    pickle.dump(results_d, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['checkpoint-1',\n",
       " 'README.md',\n",
       " 'adapter_model.safetensors',\n",
       " 'adapter_config.json',\n",
       " 'tokenizer_config.json',\n",
       " 'special_tokens_map.json',\n",
       " 'tokenizer.json',\n",
       " 'training_args.bin',\n",
       " 'train_results.json',\n",
       " 'all_results.json',\n",
       " 'trainer_state.json',\n",
       " 'comics_er_results_0.2.pickle',\n",
       " 'checkpoint-5',\n",
       " 'comics_er_results_5.pickle',\n",
       " 'checkpoint-20',\n",
       " 'comics_er_results_20.pickle',\n",
       " 'trainer_log.jsonl',\n",
       " 'checkpoint-50',\n",
       " 'comics_er_results_50.pickle']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIR, f\"\"\"comics_{TASK}_results_{NB_EPOCHS}.pickle\"\"\"), \"rb\") as fh:\n",
    "        \n",
    "        results = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounds = results[\"ground_truths\"]\n",
    "preds = results[\"predictions\"]\n",
    "\n",
    "grounds = [json.loads(x)[\"list_emotion_classes\"] for x in grounds]  \n",
    "\n",
    "# preds = [x[\"content\"] for x in preds]    \n",
    "# preds = [json.loads(x)[\"emotion_classes\"] for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grounds), len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grounds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [x[\"content\"] for x in preds]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"list_emotion_classes\": [[\"AN\", \"SA\"], [\"AN\", \"SA\"], [\"AN\"], [\"AN\", \"FE\"], [\"AN\"], [\"FE\", \"SU\"], [\"AN\"], [\"FE\", \"SU\"], [\"AN\", \"SU\"], [\"AN\", \"FE\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"AN\", \"SU\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"SU\"], [\"AN\", \"SU\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\"], [\"FE\", \"SU\"], [\"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\"], [\"SA\"], [\"AN\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\"], [\"AN\", \"FE\", \"SU\"], [\"FE\", \"SA\"], [\"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\"], [\"AN\"], [\"SU\"], [\"AN\", \"SU\"], [\"AN\", \"SU\"], [\"SU\"], [\"SU\"], [\"AN\", \"SU\"], [\"AN\", \"SU\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"SU\"], [\"AN\", \"FE\"], [\"AN\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"SA\"], [\"FE\", \"SA\"], [\"AN\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"JO\"], [\"AN\", \"JO\"], [\"FE\", \"SA\", \"JO\"], [\"SU\"], [\"AN\", \"SU\"], [\"FE\", \"SU\", \"JO\"], [\"AN\", \"FE\", \"SU\", \"JO\"], [\"SA\", \"JO\"], [\"FE\", \"SA\", \"JO\"], [\"AN\", \"SA\", \"JO\"], [\"AN\", \"FE\", \"SA\", \"JO\"], [\"SU\"], [\"AN\", \"SU\"], [\"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"JO\"], [\"AN\", \"JO\"], [\"FE\", \"SU\", \"JO\"], [\"AN\", \"FE\", \"SU\", \"JO\"], [\"SA\"], [\"FE\", \"SA\"], [\"AN\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\"], [\"AN\"], [\"SU\"], [\"AN\", \"SU\"], [\"AN\", \"SU\"], [\"JO\"], [\"SU\", \"JO\"], [\"AN\", \"SU\", \"JO\"], [\"FE\"], [\"FE\"], [\"SU\"], [\"AN\", \"FE\"], [\"AN\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"SA\"], [\"FE\", \"SA\"], [\"AN\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"JO\"], [\"AN\", \"JO\"], [\"FE\", \"SU\", \"JO\"], [\"AN\", \"FE\", \"SU\", \"JO\"], [\"SA\", \"JO\"], [\"FE\", \"SA\", \"JO\"], [\"AN\", \"SA\", \"JO\"], [\"AN\", \"FE\", \"SA\", \"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"AN\", \"SU\", \"JO\"], [\"FE\", \"SU\", \"JO\"], [\"AN\", \"FE\", \"SU\", \"JO\"], [\"SA\", \"JO\"], [\"FE\", \"SA\", \"JO\"], [\"AN\", \"SA\", \"JO\"], [\"AN\", \"FE\", \"SA\", \"JO\"], [\"SA\"], [\"FE\", \"SA\"], [\"AN\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"JO\"], [\"AN\", \"JO\"], [\"FE\", \"SU\", \"JO\"], [\"AN\", \"FE\", \"SU\", \"JO\"], [\"SA\", \"JO\"], [\"FE\", \"SA\", \"JO\"], [\"AN\", \"SA\", \"JO\"], [\"AN\", \"FE\", \"SA\", \"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"AN\", \"SU\", \"JO\"], [\"FE\", \"SU\", \"JO\"], [\"AN\", \"FE\", \"SU\", \"JO\"], [\"SA\", \"JO\"], [\"FE\", \"SA\", \"JO\"], [\"AN\", \"SA\", \"JO\"], [\"AN\", \"FE\", \"SA\", \"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"AN\", \"SU\", \"JO\"], [\"FE\", \"SU\", \"JO\"], [\"AN\", \"FE\", \"SU\", \"JO\"], [\"SA\", \"JO\"], [\"FE\", \"SA\", \"JO\"], [\"AN\", \"SA\", \"JO\"], [\"AN\", \"FE\", \"SA\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"AN\", \"',\n",
       " '{\"list_emotion_classes\": [[\"AN\"], [\"DI\"], [\"FE\"], [\"SA\"], [\"SU\"]},{\"list_emotion_classes\": [[\"AN\"], [\"DI\"], [\"FE\"], [\"SA\"], [\"SU\"]},{\"list_emotion_classes\": [[\"AN\"], [\"DI\"], [\"FE\"], [\"SA\"], [\"SU\"]}{\"list_emotion_classes\": [[\"AN\"], [\"DI\"], [\"FE\"], [\"SA\"], [\"SU\"]},{\"list_emotion_classes\": [[\"AN\"], [\"DI\"], [\"FE\"], [\"SA\"], [\"SU\"]},{\"list_emotion_classes\": [[\"AN\"], [\"DI\"], [\"FE\"], [\"SA\"], [\"SU\"]},{\"list_emotion_classes\": [[\"AN\"], [\"DI\"], [\"FE\"], [\"SA\"], [\"SU\"]},{\"list_emotion_classes\": [[\"AN\"], [\"DI\"], [\"FE\"], [\"SA\"], [\"SU\"]},{\"list_emotion_classes\": [[\"AN\"], [\"DI\"], [\"FE\"], [\"SA\"], [\"SU\"]}}{\"{\"list_emotion_classes\": [[\"AN\"], [\"DI\"], [\"FE{\"{\"list_emotion_classes\": [[\"AN\"], [\"DI\"], [\"FE\"], [\"SA\"], [\"SU\"]}{\"{\"{\"{\"{\"{\"{\"list{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"{\"',\n",
       " '{\"list_emotion_classes\": [[\"SU\"], [\"AN\", \"SU\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"AN\", \"SU\"], [\"AN\", \"SU\"], [\"SU\"], [\"SU\", \"JO\"], [\"SU\", \"JO\"], [\"SA\", \"SU\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"SU\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"SU\"], [\"SU\"], [\"SU\", \"JO\"], [\"AN\"], [\"AN\", \"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SU\"], [\"SU\"], [\"SU\", \"JO\"], [\"SU\", \"JO\"], [\"AN\"], [\"AN\"], [\"SU\"], [\"AN\", \"SU\"], [\"AN\", \"SA\", \"SU\"], [\"FE\", \"SU\"], [\"SU\", \"JO\"], [\"FE\", \"SA\", \"SU\"], [\"JO\"], [\"SU\"], [\"SU\", \"JO\"], [\"FE\", \"SU\"], [\"FE\"], [\"FE\", \"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"Neutral\"], [\"SU\"], [\"SU\", \"JO\"], [\"SA\", \"SU\"], [\"SA\"], [\"SU\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"FE\", \"SA\", \"SU\"], [\"AN\", \"FE\"], [\"FE\", \"SA\", \"SU\"], [\"SU\"], [\"JO\"], [\"SU\", \"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"SU\"], [\"JO\"], [\"SU\", \"JO\"], [\"SA\", \"SU\"], [\"SA\"], [\"SU\"], [\"SU\", \"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"Neutral\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SA\", \"SU\"], [\"SA\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"AN\", \"SU\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"FE\", \"SA\"], [\"FE\", \"SU\"], [\"SU\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"SA\", \"SU\"], [\"SA\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"AN\", \"SA\", \"SU\"], [\"AN\", \"FE\"], [\"FE\", \"SA\", \"SU\"], [\"SU\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"Neutral\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SA\", \"SU\"], [\"SA\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"Neutral\"], [\"Neutral\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"SA\", \"SU\"], [\"SA\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"Neutral\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"SU\", \"JO\"], [\"',\n",
       " '{\"list_emotion_classes\": [[\"AN\"], [\"AN\", \"SA\"], [\"SA\"], [\"AN\"], [\"AN\", \"SU\"], [\"AN\", \"SA\"], [\"FE\"], [\"AN\", \"FE\"], [\"AN\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"DI\"], [\"AN\", \"SA\"], [\"AN\", \"SU\"], [\"AN\", \"FE\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"DI\", \"SA\"], [\"AN\", \"DI\", \"SU\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"SA\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\", \"FE\"], [\"AN\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"SU\"], [\"SU\"], [\"FE\"], [\"FE\", \"SU\"], [\"AN\", \"FE\"], [\"AN\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"DI\"], [\"AN\", \"DI\", \"SU\"], [\"FE\", \"SA\"], [\"FE\", \"SU\"], [\"SA\"], [\"FE\"], [\"FE\", \"SA\"], [\"SA\", \"SU\"], [\"AN\", \"SA\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"SA\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\", \"FE\"], [\"AN\", \"DI\", \"SA\"], [\"AN\", \"DI\", \"SU\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"SA\"], [\"JO\"], [\"JO\"], [\"FE\"], [\"AN\", \"FE\"], [\"SU\"], [\"AN\", \"SU\"], [\"JO\"], [\"AN\", \"JO\"], [\"SU\"], [\"JO\"], [\"AN\", \"FE\", \"SU\"], [\"JO\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\", \"JO\"], [\"AN\"], [\"AN\", \"FE\"], [\"AN\", \"DI\"], [\"AN\", \"DI\", \"SU\"], [\"FE\", \"SA\"], [\"FE\", \"SU\"], [\"SA\"], [\"FE\"], [\"FE\", \"SA\"], [\"SA\", \"SU\"], [\"AN\", \"SA\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"SA\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\", \"JO\"], [\"AN\"], [\"AN\", \"FE\"], [\"AN\", \"DI\"], [\"AN\", \"DI\", \"SU\"], [\"FE\"], [\"AN\", \"FE\"], [\"SU\"], [\"AN\", \"SU\"], [\"JO\"], [\"AN\", \"JO\"], [\"SU\"], [\"JO\"], [\"AN\", \"SU\"], [\"JO\"], [\"Neutral\"]]}',\n",
       " '{\"list_emotion_classes\": [[\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SA\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"JO\"], [\"',\n",
       " '{\"list_emotion_classes\": [[\"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"FE\", \"SU\"], [\"FE\", \"SU\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"SU\"], [\"AN\", \"SU\"], [\"JO\"], [\"JO\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\", \"SU\"], [\"FE\", \"SU\"], [\"AN\"], [\"AN\"], [\"SU\"], [\"SU\", \"JO\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"SU\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"SU\"], [\"AN\", \"SU\"], [\"SU\", \"JO\"], [\"SU\", \"JO\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"SA\"], [\"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"SA\", \"SU\"], [\"SA\", \"SU\"], [\"SU\"], [\"SU\", \"JO\"], [\"JO\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"SA\", \"SU\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"SU\"], [\"AN\", \"FE\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"SU\", \"JO\"], [\"JO\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"SA\"], [\"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"SU\"], [\"SU\", \"JO\"], [\"JO\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"SA\", \"SU\"], [\"SA\", \"SU\"], [\"SU\", \"JO\"], [\"JO\"], [\"AN\", \"SU\"], [\"AN\", \"FE\"], [\"FE\", \"SU\"], [\"FE\", \"SA\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SA\"], [\"SA\"], [\"SA\"], [\"SU\"], [\"SU\", \"JO\"], [\"JO\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"SA\"], [\"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"SU\"], [\"SU\", \"JO\"], [\"JO\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"SA\"], [\"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"SU\"], [\"SU\", \"JO\"], [\"JO\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"SA\"], [\"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"SU\"], [\"SU\", \"JO\"], [\"JO\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"SA\"], [\"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"SU\"], [\"SU\", \"JO\"], [\"JO\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"SA\"], [\"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"SU\"], [\"SU\", \"JO\"], [\"JO\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"SA\"], [\"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"SU\"], [\"SU\", \"JO\"], [\"JO\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"SA\"], [\"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"JO\"], [\"AN\", \"FE\"], [\"AN\", \"SA\"], [\"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"SU\"], [\"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"SA\", \"SU\"], [\"SU\", \"JO\"], [\"JO\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"SA\"], [\"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"JO\"]]}',\n",
       " '{\"list_emotion_classes\": [[\"FE\"], [\"FE\"], [\"SU\"], [\"AN\", \"SU\"], [\"SA\", \"SU\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\"], [\"AN\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"FE\", \"SU\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\", \"JO\"], [\"SU\", \"JO\"], [\"FE\", \"SU\"], [\"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"SU\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"AN\", \"FE\"], [\"AN\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"FE\", \"SU\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"AN\", \"FE\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\"], [\"SU\"], [\"SU\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"AN\", \"FE\"], [\"AN\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\"], [\"SU\"], [\"SU\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"SA\"], [\"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"SA\"], [\"SA\"], [\"SU\", \"SA\"], [\"SU\", \"SA\"], [\"JO\", \"SA\"], [\"JO\", \"SA\"], [\"SU\", \"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"AN\", \"FE\"], [\"AN\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\"], [\"SU\"], [\"SU\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"SA\"], [\"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"SA\"], [\"SA\"], [\"SU\", \"SA\"], [\"SU\", \"SA\"], [\"JO\", \"SA\"], [\"JO\", \"SA\"], [\"SU\", \"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"AN\", \"FE\"], [\"AN\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\"], [\"SU\"], [\"SU\"], [\"AN\", \"SU\"], [\"AN\"], [\"AN\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"JO\"], [\"SU\"], [\"SU\"], [\"JO\"]]}',\n",
       " '{\"list_emotion_classes\": [[\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"SA\", \"SU\"], [\"AN\", \"FE\", \"SA\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"JO\"], [\"AN\", \"FE\", \"JO\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"SA\", \"SU\"], [\"AN\", \"FE\", \"SA\", \"SU\"], [\"AN\", \"JO\"], [\"AN\", \"FE\", \"JO\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"SA\", \"SU\"], [\"AN\", \"FE\", \"SA\", \"SU\"], [\"JO\"], [\"AN\", \"JO\"], [\"AN\"], [\"AN\", \"SU\"], [\"AN\", \"FE\"], [\"AN\", \"SU\"], [\"AN\", \"FE\"], [\"AN\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"SA\", \"SU\"], [\"AN\", \"FE\", \"SA\", \"SU\"], [\"JO\"], [\"AN\", \"JO\"], [\"FE\"], [\"AN\", \"FE\"], [\"SU\"], [\"AN\", \"FE\", \"SU\"], [\"SA\"], [\"AN\", \"FE\", \"SA\"], [\"JO\"], [\"AN\", \"JO\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"AN\", \"FE\"], [\"FE\"], [\"AN\", \"FE\"], [\"FE\"], [\"AN\", \"FE\"], [\"SU\"], [\"AN\", \"FE\", \"SU\"], [\"SA\"], [\"AN\", \"FE\", \"SA\"], [\"JO\"], [\"AN\", \"JO\"], [\"AN\", \"SU\"], [\"AN\", \"FE\"], [\"AN\", \"SU\"], [\"AN\", \"FE\"], [\"AN\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"SA\", \"SU\"], [\"AN\", \"FE\", \"SA\", \"SU\"], [\"JO\"], [\"AN\", \"JO\"], [\"FE\"], [\"AN\", \"FE\"], [\"FE\"], [\"AN\", \"FE\"], [\"FE\"], [\"AN\", \"FE\"], [\"SU\"], [\"AN\", \"FE\", \"SU\"], [\"SA\"], [\"AN\", \"FE\", \"SA\"], [\"JO\"], [\"AN\", \"JO\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"AN\", \"FE\"], [\"FE\"], [\"AN\", \"FE\"], [\"FE\"], [\"AN\", \"FE\"], [\"SU\"], [\"AN\", \"FE\", \"SU\"], [\"SA\"], [\"AN\", \"FE\", \"SA\"], [\"JO\"], [\"AN\", \"JO\"], [\"AN\", \"SU\"], [\"AN\", \"FE\"], [\"AN\", \"SU\"], [\"AN\", \"FE\"], [\"AN\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"SA\", \"SU\"], [\"AN\", \"FE\", \"SA\", \"SU\"], [\"JO\"], [\"AN\", \"JO\"], [\"FE\"], [\"AN\", \"FE\"], [\"AN\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"SA\"], [\"AN\", \"FE\", \"SA\"], [\"JO\"], [\"AN\", \"JO\"], [\"AN\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"SU\"], [\"AN\", \"FE\", \"SU\"], [\"',\n",
       " '{\"list_emotion_classes\": [[\"AN\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"',\n",
       " '{\"list_emotion_classes\": [[\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"SU\", \"JO\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"SU\", \"JO\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\", \"SU\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\", \"SA\"], [\"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"FE\", \"SU\"], [\"AN\", \"FE\"], [\"AN\", \"DI\", \"FE\"], [\"AN\", \"DI\", \"FE\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"AN\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SU\"], [\"AN\", \"FE\"], [\"AN\", \"DI\", \"FE\"], [\"AN\", \"DI\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"FE\"], [\"AN\", \"DI\", \"FE\"], [\"AN\", \"DI\", \"FE\"], [\"FE\"], [\"FE\"], [\"FE\"], [\"SU\"], [\"SU\"], [\"AN\", \"FE\"], [\"AN\", \"DI\", \"FE\"], [\"AN\", \"DI\", \"FE\"], [\"SU\", \"JO\"], [\"JO\"], [\"JO\"], [\"AN\", \"SU\"], [\"AN\", \"DI\", \"SU\"], [\"AN\", \"DI\", \"SU\"], [\"SA\"], [\"SA\"], [\"FE\", \"SA\"], [\"FE\", \"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"DI\", \"FE\", \"SA\"], [\"AN\", \"DI\", \"FE\", \"SA\"], [\"SU\"], [\"SU\"], [\"FE\", \"SU\"], [\"AN\", \"SU\"], [\"AN\", \"DI\", \"SU\"], [\"AN\", \"DI\", \"SU\"], [\"JO\"], [\"JO\"], [\"SU\", \"JO\"], [\"AN\", \"SU\", \"JO\"], [\"AN\", \"DI\", \"SU\", \"JO\"], [\"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"DI\", \"FE\", \"SA\"], [\"AN\", \"SU\", \"SA\"], [\"AN\", \"DI\", \"SU\", \"SA\"], [\"JO\"], [\"AN\", \"FE\", \"SA\", \"JO\"], [\"AN\", \"DI\", \"FE\", \"SA\", \"JO\"], [\"AN\", \"SU\", \"JO\"], [\"AN\", \"DI\", \"SU\", \"JO\"], [\"SA\"], [\"AN\", \"FE\"], [\"AN\", \"DI\", \"FE\"], [\"AN\", \"SU\"], [\"AN\", \"DI\", \"SU\"], [\"JO\"], [\"AN\", \"FE\", \"JO\"], [\"AN\", \"DI\", \"FE\", \"JO\"], [\"AN\", \"SU\", \"JO\"], [\"AN\", \"DI\", \"SU\", \"JO\"], [\"SU\", \"JO\"], [\"AN\", \"FE\", \"SU\"], [\"AN\", \"DI\", \"FE\", \"SU\"], [\"AN\", \"SU\", \"JO\"], [\"AN\", \"DI\", \"SU\", \"JO\"], [\"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"DI\", \"FE\", \"SA\"], [\"AN\", \"SU\", \"SA\"], [\"AN\", \"DI\", \"SU\", \"SA\"], [\"JO\"], [\"AN\", \"FE\", \"SA\", \"JO\"], [\"AN\", \"DI\", \"FE\", \"SA\", \"JO\"], [\"AN\", \"SU\", \"JO\"], [\"AN\", \"DI\", \"SU\", \"JO\"], [\"Neutral\"], [\"AN\", \"DI\"], [\"AN\", \"DI\"], [\"AN\", \"FE\"], [\"AN\", \"DI\", \"FE\"], [\"FE\"], [\"FE\"], [\"SU\"], [\"SU\"], [\"JO\"], [\"AN\", \"SU\", \"JO\"], [\"AN\", \"DI\", \"SU\", \"JO\"], [\"SA\"], [\"AN\", \"FE\", \"SA\"], [\"AN\", \"DI\", \"FE\", \"SA\"], [\"AN\", \"SU\", \"SA\"], [\"AN\", \"DI\", \"SU\", \"SA\"], [\"JO\"], [\"AN\", \"FE\", \"']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43memotion_class\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m [\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotion_class\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m preds]\n",
      "File \u001b[0;32m~/.conda/envs/llama-factory-notebook/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/.conda/envs/llama-factory-notebook/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/.conda/envs/llama-factory-notebook/lib/python3.11/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# preds = [json.loads(x)[\"emotion_class\"] for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_grounds, task_preds = post_process_acc(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check: \n",
    "len(task_preds) == len(task_grounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Claim      0.231     0.032     0.056       283\n",
      "  MajorClaim      0.724     0.136     0.230       154\n",
      "     Premise      0.661     0.997     0.795       724\n",
      "\n",
      "    accuracy                          0.648      1161\n",
      "   macro avg      0.538     0.388     0.360      1161\n",
      "weighted avg      0.564     0.648     0.540      1161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(task_grounds, task_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"\"\"{OUTPUT_DIR}/classification_report.pickle\"\"\", 'wb') as fh:\n",
    "    \n",
    "    pickle.dump(classification_report(task_grounds, task_preds, output_dict=True), fh)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
